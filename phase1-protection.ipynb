{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\n# %% [markdown]\n# #  CloakID — Phase 1: Image Immunization\n#\n# This notebook implements a **dual-layer adversarial defense** that injects\n# imperceptible noise into personal images, making them resistant to\n# manipulation by Latent Diffusion Models and Deepfake algorithms.\n\n# %% — Cell 1: Setup & Installs\n# ============================================================================\nimport subprocess, sys\nimport os\n\ndef install(pkg):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n\n# Install dependencies if not already present\nprint(\" Installing dependencies...\")\npkgs = [\"diffusers\", \"transformers\", \"accelerate\", \"gradio\", \"lpips\", \"scikit-image\"]\nfor p in pkgs:\n    install(p)\n\nprint(\" All dependencies installed.\")\n\n# %% — Cell 2: Imports & Model Loading\n# ============================================================================\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nfrom PIL import Image\nfrom skimage.metrics import structural_similarity as compare_ssim\nfrom diffusers import AutoencoderKL\nfrom transformers import CLIPModel, CLIPProcessor\nimport gradio as gr\nimport warnings, time\n\nwarnings.filterwarnings(\"ignore\")\n\n# ── Device Setup ─────────────────────────────────────────────────────────────\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Use float16 on GPU for memory efficiency, float32 on CPU\nDTYPE  = torch.float16 if DEVICE.type == \"cuda\" else torch.float32\n\nprint(f\"  Device : {DEVICE}  |  Dtype : {DTYPE}\")\n\n# ── Layer 1 Model: VAE (The Eye) ─────────────────────────────────────────────\nprint(\" Loading VAE (stabilityai/sd-vae-ft-mse) …\")\nvae = AutoencoderKL.from_pretrained(\n    \"stabilityai/sd-vae-ft-mse\",\n    torch_dtype=DTYPE,\n).to(DEVICE)\nvae.eval()\nvae.requires_grad_(False) # Freeze model\nprint(\"  VAE loaded...\")\n\n# ── Layer 2 Model: CLIP (The Brain) ──────────────────────────────────────────\nprint(\" Loading CLIP (openai/clip-vit-large-patch14) …\")\nclip_model = CLIPModel.from_pretrained(\n    \"openai/clip-vit-large-patch14\",\n    torch_dtype=DTYPE,\n).to(DEVICE)\nclip_model.eval()\nclip_model.requires_grad_(False) # Freeze model\n\nclip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\nprint(\"  CLIP loaded.\")\nprint(\" All models ready.\\n\")\n\n\n# %% — Cell 3: Core Utility Functions\n# ============================================================================\n\nATTACK_RES = 512  # Resolution used for internal gradient calculation\n\n# ── Image ↔ Tensor Helpers ──────────────────────────────────────────────────\n\ndef pil_to_tensor(pil_img: Image.Image, size: int = ATTACK_RES) -> torch.Tensor:\n    \"\"\"Convert PIL image → float32 tensor [1,3,H,W] in [0,1], resized for attack.\"\"\"\n    img = pil_img.convert(\"RGB\").resize((size, size), Image.LANCZOS)\n    arr = np.array(img).astype(np.float32) / 255.0\n    tensor = torch.from_numpy(arr).permute(2, 0, 1).unsqueeze(0)\n    # We always compute gradients in float32 for stability, even if models are fp16\n    return tensor.to(DEVICE, dtype=torch.float32)\n\n\ndef tensor_to_pil(tensor: torch.Tensor) -> Image.Image:\n    \"\"\"Convert tensor [1,3,H,W] in [0,1] → PIL image.\"\"\"\n    arr = tensor.squeeze(0).clamp(0, 1).detach().cpu().permute(1, 2, 0).numpy()\n    return Image.fromarray((arr * 255).astype(np.uint8))\n\n\ndef apply_perturbation_fullres(\n    original_pil: Image.Image,\n    delta_lowres: torch.Tensor,\n) -> Image.Image:\n    \"\"\"\n    CRITICAL FUNCTION: Solves the blurriness issue.\n    1. Takes the noise pattern (delta) calculated at 512x512.\n    2. Upscales ONLY the noise to the original image dimensions.\n    3. Adds noise to the high-res original image.\n    \"\"\"\n    orig_w, orig_h = original_pil.size\n\n    # Upscale the noise (delta) to match original image\n    # We use bilinear interpolation for the noise pattern\n    delta_fullres = F.interpolate(\n        delta_lowres,\n        size=(orig_h, orig_w),\n        mode=\"bilinear\",\n        align_corners=False,\n    )\n\n    # Convert original full-res image to tensor\n    orig_tensor = (\n        torch.from_numpy(np.array(original_pil.convert(\"RGB\")).astype(np.float32) / 255.0)\n        .permute(2, 0, 1)\n        .unsqueeze(0)\n        .to(DEVICE, dtype=torch.float32)\n    )\n\n    # Apply noise and clamp to valid range [0, 1]\n    protected = (orig_tensor + delta_fullres).clamp(0, 1)\n    \n    # Return as PIL Image\n    return tensor_to_pil(protected)\n\n\n# ── Quality Metrics ─────────────────────────────────────────────────────────\n\ndef compute_ssim(img_a: Image.Image, img_b: Image.Image) -> float:\n    \"\"\"Compute SSIM (Visual Similarity) score.\"\"\"\n    # Resize for comparison speed if image is huge\n    size = min(img_a.size[0], 1024), min(img_a.size[1], 1024)\n    a = np.array(img_a.convert(\"RGB\").resize(size, Image.LANCZOS))\n    b = np.array(img_b.convert(\"RGB\").resize(size, Image.LANCZOS))\n    return compare_ssim(a, b, channel_axis=2, data_range=255)\n\n\n# %% — Cell 4: Dual-Layer PGD Attack Engine\n# ============================================================================\n\ndef compute_vae_loss(vae_model, perturbed: torch.Tensor):\n    \"\"\"\n    Layer 1 Target: \"Gray Death\"\n    Force the VAE to generate Zero Latents (Gray/Static).\n    \"\"\"\n    # Normalize to [-1, 1] for VAE\n    perturbed_scaled = perturbed * 2.0 - 1.0\n\n    # Get Latents (Cast to model DTYPE e.g. float16)\n    pert_latent = vae_model.encode(perturbed_scaled.to(DTYPE)).latent_dist.mean\n\n    # Target: All Zeros (Gray Image)\n    target_latent = torch.zeros_like(pert_latent)\n    \n    # Loss: Minimize distance to Gray Target\n    loss = F.mse_loss(pert_latent, target_latent)\n    return loss\n\n\ndef compute_clip_loss(model, processor, perturbed: torch.Tensor):\n    \"\"\"\n    Layer 2 Target: \"Semantic Void\"\n    Push the image embedding towards a random, non-sensical vector.\n    \"\"\"\n    # CLIP Normalization constants\n    clip_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073], device=DEVICE).view(1, 3, 1, 1)\n    clip_std  = torch.tensor([0.26862954, 0.26130258, 0.27577711], device=DEVICE).view(1, 3, 1, 1)\n\n    # Resize to 224x224 for CLIP input requirements\n    img_clip = F.interpolate(perturbed, size=(224, 224), mode=\"bilinear\", align_corners=False)\n    img_clip = (img_clip - clip_mean) / clip_std\n\n    # Get features\n    image_emb = model.get_image_features(pixel_values=img_clip.to(DTYPE))\n    image_emb = image_emb / image_emb.norm(dim=-1, keepdim=True)\n\n    # -------------------------------------------------------------------------\n    # FIX: Initialize Generator on the same device as the model\n    # -------------------------------------------------------------------------\n    if DEVICE.type == \"cuda\":\n        gen = torch.Generator(device=DEVICE)\n    else:\n        gen = torch.Generator(device=\"cpu\")\n    gen.manual_seed(999) # Fixed seed for consistency\n    \n    target_emb = torch.randn(1, image_emb.shape[-1], generator=gen, device=DEVICE).to(DTYPE)\n    target_emb = target_emb / target_emb.norm(dim=-1, keepdim=True)\n\n    # Loss: Minimize distance to the Random Target\n    loss = F.mse_loss(image_emb, target_emb)\n    return loss\n\n\n@torch.enable_grad()\ndef pgd_attack(\n    original_pil: Image.Image,\n    steps: int       = 50,\n    epsilon: float   = 0.04,\n    vae_weight: float  = 1.0,\n    clip_weight: float = 0.5,\n    progress_callback  = None,\n):\n    \"\"\"\n    The Core CloakID Algorithm using Projected Gradient Descent (PGD).\n    \"\"\"\n    # Setup inputs\n    x_orig = pil_to_tensor(original_pil) # [1,3,512,512]\n    x_orig.requires_grad_(False)\n\n    # Initialize noise (delta)\n    delta = torch.zeros_like(x_orig, requires_grad=True, device=DEVICE)\n\n    # Optimization Step Size\n    alpha = epsilon / (steps * 0.4) \n\n    print(f\"\\n Starting CloakID Attack | Eps: {epsilon} | Steps: {steps}\")\n\n    for step in range(steps):\n        # Create Adversarial Image\n        x_adv = (x_orig + delta).clamp(0, 1)\n\n        # 1. Calculate Losses\n        l_vae = compute_vae_loss(vae, x_adv)\n        l_clip = compute_clip_loss(clip_model, clip_processor, x_adv)\n\n        # 2. Combined Loss\n        # We want to MINIMIZE the distance to our targets\n        total_loss = (vae_weight * l_vae) + (clip_weight * l_clip)\n\n        # 3. Calculate Gradient\n        total_loss.backward()\n\n        # 4. PGD Update (Minimize Loss)\n        with torch.no_grad():\n            grad = delta.grad.detach()\n            # Move noise towards the target\n            delta.data = delta.data - alpha * torch.sign(grad)\n            \n            # Constraint A: Epsilon (Invisibility)\n            delta.data = delta.data.clamp(-epsilon, epsilon)\n            \n            # Constraint B: Valid Pixel Range\n            delta.data = (x_orig + delta.data).clamp(0, 1) - x_orig\n\n        delta.grad.zero_()\n\n        # Progress Update\n        if progress_callback and step % 5 == 0:\n            progress_callback((step + 1) / steps, desc=f\"Optimizing... Loss: {total_loss.item():.4f}\")\n\n    # ── Final Output Generation ─────────────────────────────────────────\n    # We take the best noise pattern and apply it to the FULL RES image\n    protected_pil = apply_perturbation_fullres(original_pil, delta.detach())\n    \n    # Calculate Metrics\n    ssim_val = compute_ssim(original_pil, protected_pil)\n    \n    print(f\" Attack Complete. SSIM: {ssim_val:.4f}\")\n    \n    metrics = {\n        \"ssim\": ssim_val,\n        \"epsilon\": epsilon,\n        \"steps\": steps\n    }\n\n    return protected_pil, metrics\n\n\n# %% — Cell 5: Gradio User Interface\n# ============================================================================\n\nOUTPUT_DIR = \"/kaggle/working\"\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)\n\ndef immunize(\n    image: Image.Image,\n    intensity: float,\n    steps: int,\n    progress=gr.Progress(track_tqdm=True),\n):\n    if image is None:\n        raise gr.Error(\"Please upload an image first.\")\n\n    # Execute Attack\n    # Weights optimized for general defense (Layer 1=1.0, Layer 2=0.5)\n    protected_pil, metrics = pgd_attack(\n        original_pil = image,\n        steps        = int(steps),\n        epsilon      = intensity,\n        vae_weight   = 1.0,\n        clip_weight  = 0.5,\n        progress_callback = progress,\n    )\n\n    # Save as Lossless PNG\n    save_path = os.path.join(OUTPUT_DIR, \"cloakid_protected.png\")\n    protected_pil.save(save_path, format=\"PNG\", compress_level=1)\n\n    # Create Status Report\n    status = (\n        f\"###  Immunization Successful\\n\"\n        f\"**Visual Fidelity (SSIM):** {metrics['ssim']:.4f} \"\n        f\"{' (Good)' if metrics['ssim'] >= 0.90 else ' (Visible Noise)'}\\n\\n\"\n        f\"The image has been inoculated against Latent Diffusion models. \"\n        f\"Download the PNG below.\"\n    )\n\n    return protected_pil, status, save_path\n\n\n# ── UI Layout ───────────────────────────────────────────────────────────────\n\nwith gr.Blocks(title=\"CloakID Phase 1\", theme=gr.themes.Soft()) as demo:\n    gr.Markdown(\"#  CloakID: Image Immunization (Phase 1)\")\n    gr.Markdown(\"### Adversarial Defense Against Multimodal Diffusion Models\")\n    gr.Markdown(\"Upload your photo to apply the Dual-Layer Adversarial Defense (VAE + CLIP).\")\n\n    with gr.Row():\n        with gr.Column():\n            input_image = gr.Image(label=\"Original Image\", type=\"pil\", height=450)\n            \n            with gr.Group():\n                gr.Markdown(\"### Protection Settings\")\n                intensity_slider = gr.Slider(0.01, 0.08, value=0.04, step=0.01, label=\"Shield Strength (Epsilon)\")\n                steps_slider = gr.Slider(10, 100, value=50, step=10, label=\"Optimization Steps\")\n            \n            run_btn = gr.Button(\" Apply Immunization\", variant=\"primary\", size=\"lg\")\n\n        with gr.Column():\n            output_image = gr.Image(label=\"Protected Result\", type=\"pil\", height=450)\n            status_md = gr.Markdown()\n            download_file = gr.File(label=\" Download Lossless PNG\")\n\n    run_btn.click(\n        fn=immunize,\n        inputs=[input_image, intensity_slider, steps_slider],\n        outputs=[output_image, status_md, download_file],\n    )\n\n# %% — Cell 6: Launch\n# ============================================================================\nprint(\" Launching CloakID Interface...\")\ndemo.queue().launch(share=True, debug=True)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-13T09:15:10.041206Z","iopub.execute_input":"2026-02-13T09:15:10.041842Z","iopub.status.idle":"2026-02-13T09:17:19.965683Z"}},"outputs":[{"name":"stdout","text":" Installing dependencies...\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 68.6/68.6 kB 2.1 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 444.8/444.8 kB 11.3 MB/s eta 0:00:00\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 54.0 MB/s eta 0:00:00\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nlangchain-core 0.3.79 requires packaging<26.0.0,>=23.2.0, but you have packaging 26.0rc2 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\n","output_type":"stream"},{"name":"stdout","text":"   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.8/53.8 kB 2.3 MB/s eta 0:00:00\n All dependencies installed.\n","output_type":"stream"},{"name":"stderr","text":"2026-02-13 09:15:50.939175: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1770974151.110706      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1770974151.160737      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1770974151.594099      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770974151.594142      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770974151.594144      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1770974151.594147      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"  Device : cuda  |  Dtype : torch.float16\n Loading VAE (stabilityai/sd-vae-ft-mse) …\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/547 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa1c18ab9bc546c4886f5a9c0276e35f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"diffusion_pytorch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"866ae62dc43b4cf6b807bd53c80bdf30"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"name":"stdout","text":"  VAE loaded...\n Loading CLIP (openai/clip-vit-large-patch14) …\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eba026e2bfc1480ca5c43199ba5cbfbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.71G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1cd70173798463db76dea3de492a392"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25de8a4e48c541fea65c86ddbb289ce1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/905 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab1d87e7019442899271db0a70b32570"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87f4646851b94a1897fcb281e950d3d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53468aaab80c48458ca99803259689ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"727a0d0e4a464fef89d5d789b351b27d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60c7fa9560db4059bab551ddd23bbcec"}},"metadata":{}},{"name":"stdout","text":"  CLIP loaded.\n All models ready.\n\n Launching CloakID Interface...\n* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://6017015508a7e01bb6.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://6017015508a7e01bb6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stdout","text":"Keyboard interruption in main thread... closing server.\nKilling tunnel 127.0.0.1:7860 <> https://6017015508a7e01bb6.gradio.live\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":null}]}